"""Utilities for constructing deterministic ExpDisaster episodes.

This module converts a pre-generated ExpDisaster split JSON into a list of
deterministic ``EpisodeSpec`` objects that the dataset can consume.

Supported input format (generated by ``script/generate_disaster_splits.py``)::

    {
        "support": {"images": [...], "labels": [...]},
        "query":   {"images": [...], "labels": [...]}
    }

Notes
-----
- This parser does not support legacy formats (e.g., ``support_set/query_set``).
- Filenames in JSON are treated as paths to images/labels for analysis; when
  building episodes we pass image filenames through unchanged. The dataset
  matches by basename stem, so absolute/relative paths are acceptable.
"""

from __future__ import annotations

import json
from pathlib import Path
from typing import Dict, Iterable, List, Sequence, Tuple

import numpy as np
import rasterio

from dataloaders.exp_disaster_fewshot import EpisodeSpec


def _resolve_to_abs(path: str, base: Path) -> Path:
    """Resolve JSON-listed path to an absolute ``Path`` based on ``base``."""
    p = Path(path)
    return (base / p).resolve() if not p.is_absolute() else p


def _chunk(sequence: Sequence[str], size: int) -> Iterable[List[str]]:
    """Yield successive chunks from ``sequence`` with length ``size``."""
    if size <= 0:
        raise ValueError("Chunk size must be positive.")
    for idx in range(0, len(sequence), size):
        chunk = list(sequence[idx : idx + size])
        if len(chunk) < size:
            break
        yield chunk


def episode_specs_from_split(
    split_path: str | Path,
    *,
    n_ways: int,
    n_shots: int,
    n_queries: int,
    class_remap: Dict[int, int],
    ignore_label: int,
) -> List[EpisodeSpec]:
    """Convert a new-format ExpDisaster split into deterministic episodes.

    Expected JSON structure (only this format is supported)::

        {
            "support": {"images": [...], "labels": [...]},
            "query":   {"images": [...], "labels": [...]}
        }

    For each remapped foreground class present in support labels, we:
    - select the first ``n_shots`` support images that contain the class;
    - collect all query images that contain the class;
    - generate episodes by chunking those queries in groups of ``n_queries``.
    """
    path = Path(split_path)
    with path.open("r", encoding="utf-8") as handle:
        data = json.load(handle)

    if not isinstance(data, dict):
        raise ValueError(
            f"Unsupported split file format at '{path}'. Expected a JSON object."
        )

    def _get_list(d: Dict, key: str) -> List[str]:
        if key not in d or not isinstance(d[key], list):
            raise ValueError(
                f"Key '{key}' missing or not a list in '{path}'."
            )
        return list(d[key])

    if "support" not in data or "query" not in data:
        raise ValueError(
            f"Split JSON must contain 'support' and 'query' objects: '{path}'."
        )

    support_images = _get_list(data["support"], "images")
    support_labels = _get_list(data["support"], "labels")
    query_images = _get_list(data["query"], "images")
    query_labels = _get_list(data["query"], "labels")

    if len(support_images) != len(support_labels):
        raise ValueError(
            f"Support images/labels length mismatch in '{path}': "
            f"{len(support_images)} vs {len(support_labels)}."
        )
    if len(query_images) != len(query_labels):
        raise ValueError(
            f"Query images/labels length mismatch in '{path}': "
            f"{len(query_images)} vs {len(query_labels)}."
        )

    # Build reverse map from JSON paths to absolute paths for label reading
    base = Path.cwd()

    def _read_label_abs(label_path: str) -> np.ndarray:
        lp = _resolve_to_abs(label_path, base)
        with rasterio.open(lp) as src:
            label = src.read(1)
            nodata = src.nodata
        # remap raw -> episode ids
        lut = np.full(256, ignore_label, dtype=np.int16)
        for raw, mapped in class_remap.items():
            if 0 <= int(raw) < len(lut):
                lut[int(raw)] = int(mapped)
        remapped = lut[label]
        if nodata is not None:
            remapped[label == nodata] = ignore_label
        return remapped

    def _present_classes(remapped: np.ndarray) -> List[int]:
        vals = set(np.unique(remapped).tolist())
        vals.discard(0)
        vals.discard(ignore_label)
        return sorted(int(v) for v in vals)

    # Aggregate per-class support images and query images based on labels
    support_by_class: Dict[int, List[str]] = {}
    for img_path, lbl_path in zip(support_images, support_labels):
        remapped = _read_label_abs(lbl_path)
        for cid in _present_classes(remapped):
            support_by_class.setdefault(cid, []).append(img_path)

    query_by_class: Dict[int, List[str]] = {}
    for img_path, lbl_path in zip(query_images, query_labels):
        remapped = _read_label_abs(lbl_path)
        for cid in _present_classes(remapped):
            query_by_class.setdefault(cid, []).append(img_path)

    if n_ways != 1:
        raise NotImplementedError(
            "Only n_ways=1 is supported by the fixed-split episode builder."
        )

    classes = sorted(support_by_class.keys())
    if not classes:
        raise ValueError(
            f"No foreground classes discovered from support labels in '{path}'."
        )

    episode_specs: List[EpisodeSpec] = []
    for class_id in classes:
        support_files = support_by_class.get(class_id, [])
        if len(support_files) < n_shots:
            raise ValueError(
                f"Class {class_id} has only {len(support_files)} support images; "
                f"need at least {n_shots} (file: '{path}')."
            )
        query_files = query_by_class.get(class_id, [])
        if len(query_files) < n_queries:
            raise ValueError(
                f"Class {class_id} has only {len(query_files)} query images; "
                f"need at least {n_queries} (file: '{path}')."
            )

        support_subset = support_files[:n_shots]
        for query_chunk in _chunk(query_files, n_queries):
            episode_specs.append(
                EpisodeSpec(
                    class_ids=[class_id],
                    support={class_id: support_subset},
                    query=query_chunk,
                )
            )

    if not episode_specs:
        raise ValueError(
            f"No episode specifications generated from '{path}'."
        )

    return episode_specs


def episode_specs_from_dicts(entries: Sequence[Dict]) -> List[EpisodeSpec]:
    """Convert a list of dictionaries into concrete ``EpisodeSpec`` objects."""
    episode_specs: List[EpisodeSpec] = []
    for entry in entries:
        if "class_ids" not in entry:
            raise ValueError("Each episode spec must contain 'class_ids'.")
        episode_specs.append(
            EpisodeSpec(
                class_ids=entry["class_ids"],
                support={int(k): v for k, v in entry.get("support", {}).items()},
                query=entry.get("query", []),
            )
        )
    return episode_specs
